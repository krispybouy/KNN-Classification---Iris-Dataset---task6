# K-Nearest Neighbors (KNN) Classification

In this task, I worked on implementing the K-Nearest Neighbors algorithm to classify data points based on their similarity to nearby examples.

## What I Did
1. Loaded the local Iris dataset and explored its structure.  
2. Encoded the categorical `Species` column into numeric values.  
3. Normalized all feature columns to make distance-based calculations fair.  
4. Trained a KNN classifier using Scikit-learn.  
5. Evaluated model performance with accuracy, confusion matrix, and classification report.  
6. Experimented with different values of K to find the one giving the best accuracy.  
7. Visualized decision boundaries using two selected features.


## Tools Used
- Python  
- Pandas  
- NumPy  
- Matplotlib  
- Seaborn  
- Scikit-learn  

## Key Learnings
- Normalization is important for distance-based algorithms like KNN.  
- The choice of K affects how sensitive the model is to noise and generalization.  
- Visualization helps understand how KNN separates different classes.  
- KNN is simple but performs well when data is clean and properly scaled.

This task helped me understand how KNN works intuitively and how to tune it effectively for better performance.
